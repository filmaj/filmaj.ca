---
title: Testing PhoneGap Apps
description: Testing cross-platform PhoneGap applications to automate the labour away
---
<section data-background="images/phonesmash.gif">
<h1>Testing PhoneGap Apps</h1>
<h3>Automating the Pain Away</h3>
<p>
<small><a href="http://filmaj.ca">Fil Maj</a> / <a href="http://twitter.com/filmaj">@filmaj</a></small>
</p>
</section>

<section>
<h2>Agenda</h2>
<ol style="font-size:85%;">
<li>The Why (~10 mins)
<ul>
<li>Bugs and Regressions</li>
<li>Safety Net</li>
<li>Goal: Continuous Integration</li>
</ul>
</li>
<li>The How (~45 mins)
<ul>
<li>Overview of Different Testing Approaches:
<ul>
<li>Unit Testing (~20 mins)</li>
<li>UI Testing (~20 mins)</li>
</ul>
</li>
<li>Automating Testing with Continuous Integration</li>
</ul>
</li>
<li>free-roam workshop time: hack, code, get help, show off demos</li>
</ol>
</section>

<section data-background="images/filmaj.jpg">
<h1>Ohai! I'm Fil</h1>
<img src="images/PhoneGap-Symbol-white.png" style="border:0" />
</section>

<section>
<h2>Ohai! who r u?</h2>
<p>Designer? Developer? QA?</p>
<aside class="notes">
what kind of a group do we have here?
</aside>
</section>

<section data-background="images/why_test.png" data-background-size="800px">
<h1>Why Test?</h1>
<h3>Audience Poll: Who does it?</h3>
<p class="fragment" >Dedicated QA department?</p>
<p class="fragment">Developers?</p>
<p class="fragment">Unclear?</p>
<aside class="notes">
presumably we're all involved in shipping some kind of product
do you test, and if so, who does it?
</aside>
</section>

<section data-background="images/why_test.png" data-background-size="800px">
<h1>Why Test?</h1>
<h3>Audience Poll: What kind of testing?</h3>
<p class="fragment">Manual?</p>
<p class="fragment">Unit?</p>
<p class="fragment">Integration / End-to-End / Selenium?</p>
<aside class="notes">
what sorts of testing are you/your team doing? something not here?
</aside>
</section>

<section data-background="images/quality.jpg">
<h3 style="margin-top:500px;">Quality is everyone's responsibility.</h3>
<aside class="notes">
if there's one thing to leave this workshop with, it's this message
</aside>
</section>

<section data-background="images/bug.jpg">
<div style="background-color:rgba(0,0,0,0.5);">
<h1>Bugs</h1>
<p>They suck. We have to fix them.</p>
<p>There's something worse than just a <em>bug</em>, though...</p>
</div>
<aside class="notes">
when i think poor quality software, bugs are what come to mind
</aside>
</section>

<section>
<h2>Regressions</h2>
<h5>A bug that, at one point, was identified and fixed, but at a later date, crept back into the product.</h5>
<p>One of the biggest embarassments as a team delivering a product.</p>
<aside class="notes">
by sticking to a few principles and following some of the techniques i hope y'all will learn in this workshop,
you can avoid regressions altogether.
</aside>
</section>

<section>
<h3>Preventing Regressions</h3>
<ul>
<li class="fragment">You release version 1 of your product. Yay!</li>
<li class="fragment">You realize there's a bug in it. Boo...</li>
<li class="fragment">
You fix the bug, but then you also write a little program to verify that the bug doesn't exist.
<br/>
<em class="fragment">This little program is a test!</em>
</li>
<li class="fragment">You release version 2 of your product, which includes your bug fix. Yay!</li>
<li class="fragment">Now every time you make a change, you run your test, <em>effectively preventing you from ever releasing that same bug again</em>.</li>
</ul>
<p class="fragment">LMAO, RIP, Bug</p>
</section>

<section data-background="images/net.jpg">
<h2>The Power of Testing</h2>
<p>Testing is a safety net.</p>
<p>You are future-proofing your product.</p>
<p class="fragment">It is also a powerful design tool - it lets you reason and formalize your architecture.</p>
<aside class="notes">
weve talked about how testing can help us incrementally improve the quality of our software, and prevent regressions.
but what about testing as a way to shape software in the first place? who has heard of Test Driven Development?
consider how your test suite would evolve differently, and how that shapes your program, if every test added was
from a bug to prevent a regression, vs. a meaningful test describing a high level feature.
"it should not *due to specific bug behaviour*"
"it should interact with module X in this particular way"
</aside>
</section>

<section>
<h3>Continuous Integration</h3>
<img src="images/hudson.png" />
<h4>Next-Level Testing Power Up: The Holy Grail</h4>
<p class="fragment">Your tests being <em>automated</em> - fire and forget</p>
<p class="fragment">Your automated test suite running <em>on every commit</em>.</p>
<p class="fragment">You get feedback very quickly about your changes - did they break the tests?</p>
<p class="fragment">Workshop Goal: instrument a PhoneGap app a) with tests and then b) automate them</p>
</section>

<section>
<h2>How to Test</h2>
<p>So many ways to approach testing your product - as many ways as there are to building it.</p>
</section>

<section>
<h2>Testing Approaches</h2>
<p>Different ways of experiencing and slicing your product up:</p>
<ul>
<li class="fragment"><strong>Manual</strong>. Slow, error prone, human required.</li>
<li class="fragment"><strong>Unit</strong>. Testing code at the granular unit/module/class level.</li>
<li class="fragment"><strong>Visual</strong>. Does it look like what it's supposed to look like?</li>
<li class="fragment"><strong>Integration</strong>. Once you put all the pieces together, does it work?</li>
</ul>
<p class="fragment">Today we will be focussing on unit testing and integration/end-to-end testing specifically for PhoneGap apps.</p>
</section>

<section>
<h2>Before we begin...</h2>
<p>We will be testing a PhoneGap sample app called Star Track.</p>
<p><a href="http://github.com/phonegap/phonegap-app-star-track">http://github.com/phonegap/phonegap-app-star-track</a></p>
<p>Let me demo it.</p>
<p class="fragment">
I have a branch of the sample app up on my fork that has all the various testing bits included. I will be walking through that code and demoing it throughout the workshop. Grab it if you don't already have it!
<a href="https://github.com/filmaj/phonegap-app-star-track/tree/workshop-2017">https://github.com/filmaj/phonegap-app-star-track/tree/workshop-2017</a>
</p>
</section>

<section data-background="images/unit-test-isolation.png">
<div style="background-color:rgba(0,0,0,0.75);">
<h2>Unit Testing</h2>
<p>Testing the building blocks that make up your app, one block (or <em>unit</em>) at a time.</p>
<p class="fragment">The driving assumption is that the majority of quality problems in your application come from <em>your</em> code/logic.</p>
<h3 class="fragment">Therefore, we focus unit tests on your application's logic.</h3>
</div>
<aside class="notes">
should be focused on JUST the logic of your units. so, really disassembling your app into smaller parts we can test individually.
its expected that the programming languages / frameworks / programs you leverage as dependencies that make up your app (phoengap, cordova, the browser) are well tested and works. the assumption is that the main contributors to errors/failure in your app is YOUR code. so focus on testing YOUR code first and foremost.
</aside>
</section>

<section data-background="images/unit-test-isolation.png">
<div style="background-color:rgba(0,0,0,0.75);">
<h2>Unit Testing, cont.</h2>
<p>As the first line of defense against bugs and regressions, we want unit tests to:</p>
<ul>
<li class="fragment">Be easy to run, since we will run them all the time during development.</li>
<li class="fragment">Be fast. Slow unit tests make me less likely to run them.</li>
</ul>
</div>
<aside class="notes">
the second part (fast unit tests) actually enforces the principle of isolation: any kind of i/o will slow your test.
</aside>
</section>

<section data-background="images/unit-test-isolation.png">
<div style="background-color:rgba(0,0,0,0.75);">
<h2>How Not to Unit Test</h2>
<img src="images/unit-test-death.png" />
</div>
<aside class="notes">
the previous two principles around unit tests leads certain constraints that are helpful to hold yourself to
no i/o (file/network)!!!!! if your unit test does network calls, its not a unit test, because we violate the principle of isolation.
</aside>
</section>
<section data-background="images/unit-test-isolation.png">
<div style="background-color:rgba(0,0,0,0.75);">
<h2>Unit Testing a PhoneGap Application</h2>
<p class="fragment">A PhoneGap app is composed of HTML, JavaScript and CSS.</p>
<p class="fragment">The logic in your code mostly exists in JavaScript.</p>
<h4 class="fragment">So for your PhoneGap application, we will be unit testing JavaScript.</h4>
</div>
</section>

<section>
<h2>Quick Look at Star Track JavaScript</h2>
<p><a href="https://github.com/phonegap/phonegap-app-star-track/blob/master/www/js/my-app.js">https://github.com/phonegap/phonegap-app-star-track/blob/master/www/js/my-app.js</a></p>
<aside class="notes">
this is a single-page app, and all of the app's logic is contained within a single JS file. if your app's source is split among multiple files, the same approach I will be dtailing applies to you, too. the single-js-file scenario is generally trickier to test properly than a more modular approach.
</aside>
</section>

<section>
<h2>So How Do We Unit Test This?</h2>
<p>Three requirements:</p>
<ol>
<li class="fragment">We need an environment where we can load our application's JavaScript and execute it.</li>
<li class="fragment">We need an environment where we can load our tests and execute them.</li>
<li class="fragment">We need the ability to report the test results.</li>
</ol>
</section>

<section>
<img src="images/karma-banner.png" style="border:0"/>
<p>A versatile JavaScript test runner that meets our three requirements, and works with any testing library.</p>
<p><a href="http://karma-runner.github.io">karma-runner.github.io</a></p>
<p>Refer to Karma's site for installation, or just jump straight to my fork of the Star Track app.</p>
</section>

<section>
<img src="images/karma-banner.png" style="border:0"/>
<h2>Requirement 1</h2>
<h4>Environment for our JavaScript</h4>
<p class="fragment">To start, follow the installation guide to get all the Karma pieces in place - let's run through it quickly.</p>
<p class="fragment">Let me walk through the Karma config to explain how Karma loads our app's JS.</p>
<aside class="notes">
cover basepath, files, browsers, autowatch and single run. we will turn autowatch on and single run off so that we can
focus/optimize on the app development experience. that is, as an app developer making changes to my code, how do i streamline
the test-running experience to help my development efforts. i will show that in a sec.
remember that the order of the files we include matter. in this case, our app relies on Framework7. so we need to include framework7
FIRST - just like in the browser! similarly, our tests rely on our own app code, so our tests get included last.
another common gotcha is coupling the logic (JS) to the view (HTML/document). some simple tweaks to the basic structure of our app let us
decouple the javascript from the document almost entirely. let me show you how that's done in the star track app case - there is a single init()
call that kicks off finding DOM elements, binding to their events, that kinda stuff.
</aside>
</section>

<section>
<img src="images/karma-banner.png" style="border:0"/>
<h2>Requirement 2</h2>
<h4>Environment for our Tests</h4>
<p class="fragment">Now let's take a look at some tests!</p>
<p class="fragment">Let me walk through the Karma config to explain how Karma loads our tests.</p>
<aside class="notes">
open up test file. series of describe blocks that group relevant tests together, and it blocks that encapsulate individual tests.
i like to organize describe blocks by module or function that i am testing, and have each test exercise one aspect of my code.
open up karma config again. show frameworks. we're using jasmine, so we tell karma "expect to run jasmine tests now", k?
</aside>
</section>
<section>
<img src="images/karma-banner.png" style="border:0"/>
<h2>Requirement 3</h2>
<h4>Reporting Test Results</h4>
<p class="fragment">Node.js is our main runtime, so let's create easy <code>npm</code> scripts to run our tests.</p>
<aside class="notes">
show package.json with dependencies for all karma things, and the one npm script we added `npm run unit`. all it does is trigger karma
</aside>
</section>
<section>
<h2>Let's give it a shot!</h2>
<aside class="notes">
sweet! it worked. we ran 7 tests and got feedback that they passed - that's nice. also note that if i change the tests or the source code,
karma runs the tests immediately in the background for you. this is a nice addition to one's development flow. let's zoom in on the pad2
function and tests to show this off.
</aside>
</section>
<section data-background="images/mock.jpg">
<h2>Essential Unit Testing Tool: Mocking</h2>
<p class="fragment">Recall that unit testing relies on isolating our logic to small units.</p>
<p class="fragment">This can make executing code that is <strong>highly coupled</strong> to other units difficult.</p>
<p class="fragment"><strong>Mocking</strong> is a technique where we use software dummies to stand-in for other units, dependencies or even function parameters in order to keep tests <em>focussed</em>. </p>
<aside class="notes">
also called stubbing. lets look at the second set of tests that depict this technique, in increasingly more complex unit tests. the second set of tests exercise the javascript function responsible for doing the search: it'll validate the input, make an API call to spotify to search for music, delegate rendering of the results to another function, and finally handle any errors during the call to the spotify api. That's at least four tests, if not more.
</aside>
</section>
<section>
<h2>End-to-End Testing</h2>
<p>Also known as integration testing...</p>
<p>... and UI testing...</p>
<p>.. and other terms - but the idea is always the same:</p>
<h3 class="fragment">Treat the software-under-test as a single, whole package, and interact with it as your customers do.</h3>
<aside class="notes">
remember the assumption that failures are caused by your code, first and foremost? while that may be true 90% of the time, there are times when that assumption does not hold. where the interactions between the dependencies of your code, your code itself, and anything else cause weird behaviour.
</aside>
</section>
<section>
<h2>End-to-End Testing, cont.</h2>
<p>This kind of testing used to be done <strong>manually</strong>.</p>
<img src="images/manual-testing.gif" />
</section>
<section>
<h2>End-to-End Testing, cont.</h2>
<p>What we want is an <strong>automated</strong> way to replace UI interactions.</p>
<p>In that sense, it is the opposite of unit testing with <em>mocking</em>:</p>
<ul>
<li class="fragment">No disassembling our code into small <em>units</em> we test individually - instead we test the entire application.</li>
<li class="fragment">No use of mocks - bring on the I/O.</li>
</ul>
<aside class="notes">
integration testing treats your entire application as one giant package, with everything included: UI, integrations with other API/services. in other words, _nothing_ is mocked - which is completely the opposite from what we did during unit testing. that introduces its own challenges, but the benefit is we are testing and playing with a version of the software that is identical, or very close, to what our users will be using.
HEAVY! expensive, slow. if you dont need to do integration testing, dont do it. generally, the larger the project and the longer it has been around, the more likely you need integration testing.
</aside>
</section>

<section>
<h2>Selenium</h2>
<h4>(aka WebDriver)</h4>
<p>Selenium is a tool that enables automation. It tells other software to do things for you.</p>
<aside class="notes">
who has heard of selenium? uses it?
for web-based software projects, there is a web standard called WebDriver that allows us to automate UIs. it provides a way to load URLs, tap/click elements, and validate that the web app is in a particular state. not that different from our JS unit tests, except that we’re no longer isolating the JS
</aside>
</section>

<section>
<h2>What is WebDriver?</h2>
<p class="fragment">It is commonly referred to as Selenium (for maximum confusion)</p>
<p class="fragment">It is a <strong>specification</strong> for an HTTP API.</p>
<p class="fragment">This HTTP API serves as an <em>abstraction</em> over proprietary, closed and/or differing UI automation APIs.</p>
<p class="fragment">Free and open-source.</p>
<aside class="notes">
where phonegap provides a JS API for various native device functionalities, selenium works in the _exact_ same way for automation APIs.
</aside>
</section>

<section data-background="#dddddd">
<h2 style="color:black;">How Selenium Works</h2>
<img src="images/selenium_diagram.png" />
<aside class="notes">
very briefly cover how this works, and we will segue into how we will use it.
four components: the thing you want to automate, the driver (an API to automate the thing), the selenium server, which is an abstraction over the driver, and your test, which speaks to the server.
</aside>
</section>

<section data-background="images/appium.png" data-background-size="400px">
<div style="background-color:rgba(0,0,0,0.5);">
<h3>What is Appium?</h3>
<p class="fragment">An implementation (+extension) of the WebDriver specification...</p>
<p class="fragment">... designed specifically for mobile (Android, iOS) platforms...</p>
<p class="fragment">... allowing automation of native, hybrid and web applications.</p>
</div>
</section>

<section>
<h2>Back to our App...</h2>
<img src="images/nightwatch.png" />
<p>Similar to Karma for unit testing, Nightwatch is a test runner purpose-built for integration testing in a browser.</p>
<aside class="notes">We will use Nightwatch to glue our integration testing setup together</aside>
</section>

<section data-background="images/nightwatch.png">
<div style="background-color:rgba(0,0,0,0.5);">
<p>Let's set it up.</p>
<p><a href="http://nightwatchjs.org/gettingstarted">http://nightwatchjs.org/gettingstarted</a></p>
</div>
<aside class="notes">
We will use Nightwatch to glue our integration testing setup together
first, let’s install it, and something called a selenium-server and chromedriver: npm install nightwatch selenium-server chromedriver —save-dev.
our first integration testing environment will be a browser (chrome, in this case). and, just like how in our development loop / workflow, we test
often using `phonegap serve` and loading up the browser, we will leverage this same approach to run integration tests in chrome.
this will be a stepping stone to running these integration tests on mobile.
lets go over the nightwatch config to see whats in there. lets cover the basic setup, the selenium server and chromedriver bits, and the default test environment.
</aside>

</section>
<section>
<p>We need one more bit of glue: a <code>runner.js</code> script.</p>
<p class="fragment">It will handle running <code>phonegap serve</code> for us, and kicking off nightwatch. Let's take a look.</p>
<aside class="notes">
we also need to write what im calling a ‘runner’ script - something that gives us the flexbility to set up the testing environment in such a way that it works for our phonegap app. in particular, the runner script will run `phonegap serve` for us, and once its running, only then will it kick off nightwatch.
it will also help run some extra processes to test inside mobile emulators, but i will show that off in a bit.
</aside>
</section>
<section>
<h2>Web Integration Test</h2>
<p>Let's take a look at what integration tests in this case look like.</p>
<aside class="notes">
writing our first test. simple: load the url, make sure the find tracks button is present.
</aside>
</section>
<section>
<h2>Let's give it a shot!</h2>
</section>
<section>
<h2>What about Mobile?</h2>
<p>That Appium thing we talked about earlier - how do we get it to run our tests in a mobile environment?</p>
</section>
<section data-background="images/appium.png" data-background-size="400px">
<div style="background-color:rgba(0,0,0,0.5);">
<h3>Appium</h3>
<p>But! The browser is not the same as a PhoneGap (hybrid) application:</p>
<p class="fragment">Don't need to manage URLs.</p>
<p class="fragment">Differentiating between "web" vs. "native" contexts.</p>
</div>
<aside class="notes">
even though the automation API is identical, the app running inside a browser vs running inside a native app container are two vastly different contexts. two main differences:
1. there is no concept of URL loading in a cordova app. so explicitly loading the URL is not needed, and in fact would probably fail.
2. phonegap apps are native apps, but your app lives inside what is called a webview within the native application. so when automating the UI of a phonegap app, we need to have the tests find this webview and _change the context of the UI automation to the webview.
</aside>
</section>
<section data-background="images/appium.png" data-background-size="400px">
<div style="background-color:rgba(0,0,0,0.5);">
<h3>Appium, cont.</h3>
<p>We can easily factor out this difference in environments using a <code>before</code> clause in our tests. Let's take a look.</p>
</div>
<aside class="notes">
so we need to tweak the tests such that the browser context and the native app context are both handled. we can do so by factoring out these differences into test scripts that run before/after all the tests, or each of the tests.
lets also show the different mobile environments we are testing in the nightwatch config, and how our runner script will start appium for us. note how we are pointing our test runner to the final built native binaries - we are literally testing what we eventuall ship to our customers.
worth noting that we probably want to build a new mobile binary before running the tests - to package up any changes and test those, too.
</aside>
</section>

<section data-background="images/appium.png" data-background-size="400px">
<div style="background-color:rgba(0,0,0,0.5);">
<h3>Let's Give it a Run!</h3>
</div>
</section>

<section>
<h3>Continuous Integration</h3>
<p>Classic software development lifecycle:</p>
<ol>
<li>Develop - <strong>manual</strong></li>
<li>Test - <strong>manual</strong></li>
<li>Release - <strong>manual</strong></li>
</ol>
</section>
<section>
<h3>Continuous Integration, cont.</h3>
<p>Continuous integration-powered development lifecycle:</p>
<ol>
<li>
<ul>
<li>Develop - <strong>manual</strong></li>
<li>Test - <strong>automated</strong> - run <em>while</em> you develop</li>
</ul>
</li>
<li>Release - <strong>manual</strong></li>
</ol>
<aside class="notes">
ideally the people involved in app development are running all these awesome testing tools we’ve talked about locally. but in a big team, sometimes we forget to do that, or people’s environments are slightly different than our own or our customers’.
the idea is to automate the _running_ of all of these things we’ve talked about today on every change we make to our app.
so investing a little bit of time in automating the running of tests and checks on every commit can go a long way to saving time.
the karma unit tests we ran earlier, and its autowatch/autorun features, is a great example of tightening up the development and testing phases of the software development lifecycle, speeding up delivery.
you can take it even one step further and automate the releasing of your application to achieve Continuous Deployment - where every phase of the SDL other than development itself is automated. quite common these days with web applications and services. less so with mobile apps, mostly due to typical app store review processes.
</aside>
</section>
<section>
<h2>CI Details</h2>
<img src="images/branching.png" />
<p>Suggest to automate running your tests and checks on every commit to a couple of different typical source code version control locations:</p>
<ul>
<li class="fragment">On the <code>master</code> branch (or whatever branch used to reflect production or release-candidates)</li>
<li class="fragment">On Pull Request (or ready-to-review) branches</li>
</ul>
<aside class="notes">
id suggest running the tests on two different code change events/branches/locations:
1. master, or whatever branch you use with your team that denotes “what is in production”.
2. pull requests. PRs are a medium in which someone signals intent to change the software - perfect opportunity to run and report test results to.
it might seem like overkill, but in git, merge commits can sometimes include unintentional changes, so having one extra run on master over and above a PR can help catch those. PLUS: this is automated. a computer will do this work for us. it is mindless to me and you. all it takes is some compute time in the cloud. so why not?
</aside>
</section>
<section data-background="images/travis.png" data-background-size="600px">
<div style="background-color:rgba(0,0,0,0.6);">
<h2>Travis</h2>
<p>A simple, free and beautiful service for enhancing software project with automation - perfect for CI!</p>
<p class="fragment">Let's walk through setting up Travis for our app.</p>
</div>
<aside class="notes">
who is familiar with Travis?
a simple, free, and beautiful tool called travis is free for public repos. let me show you how to set it up. you log into travis with your github, enable travis runs for your repo of choice, tweak some settings (go over them), and then write up a travis.yml file (go over the basics). push those changes up, and travis will start executing the tests for you!
if we want to run karma on every change, we have to make sure to run it not in the ‘watch’ mode, like we did in the earlier unit testing section. we want to run it once, and report back.
it’s super useful when hook into PRs, too. you get provided that feedback at code-review and code-sharing time.
so lets put together a pull request that _breaks_ the tests, and have travis report it back to us.
</aside>
</section>
<section>
<h2>Code Coverage</h2>
<img src="images/codecov.png" />
<p>A measure of how much of your code is being executed by your tests.</p>
<p class="fragment">Don't waste your time forcing this to 100%! It's just another data point. Good goal: continuous improvement, make it generally go up, not down.</p>
<p class="fragment">Let me show you how to hook it up to Karma and see it in action.</p>
<p class="fragment">You can also easily hook it into your PR process, just like Travis, using <a href="http://codecov.io">http://codecov.io</a>.</p>
<aside class="notes">
another useful little extra tool you can hook into your dev cycle is code coverage reporting. its a metrics that tells you what % of your code your tests are exercising.
its a rough metric, and doesnt tell you much other than how much of your code you are testing. dont chase this metric too much! code coverage does not tell you about the quality of your tests, just how much of your code is being exercised. human judgment still needed. i find it is useful to know how code coverage _changes_ as code changes. does a pull request drop code coverage, or increase it? if coverage is dropped, perhaps thats one thing to look at in the PR: does it have sufficient test coverage? were tests written at all?
let me show you to get up and running with one such tool kinda already built into karma:
—> npm install karma-coverage codecov —save-dev. editing karma config to show a summary of that on every run (via reporter). needs to preprocess your js files (to figure out the coverage metrics). run `npm run unit` and check the report out at the bottom.
you can also have travis run codecoverage and report it back to us. useful, again, in PRs! show .travis.yml.
lets demo.
</aside>
</section>

<section data-background="images/hacking.jpg">
<div style="background-color:rgba(0,0,0,0.5);">
<h2>Let's Get Hacking!</h2>
<p>Suraj and I will be wandering around, answering questions, helping with set up or issues, and generally available for the last bit of the workshop.</p>
</div>
</section>

<section>
<h3>Image Attribution</h3>
<p><a href="https://www.flickr.com/photos/dieselbug2007/414348333/in/photolist-CBDjM-pyfoz4-f12vq8-nPs6XQ-uZLGbg-6cgNzy-pNEHMV-dBQxKE-bCahK9-czYRPu-ekT1vr-dHTAp1-o6WHCk-o6DioD-FHtBo-o4TNGs-565kyS-nPsjPR-nPsjew-o6QRrq-o6WGGT-o6PLwQ-kdAbds-9an5xh-bevNHZ-bbSRU-gZXNNm-nHS9VZ-qKoZzC-nPsiVW-bcbQ3i-4epuVH-hhufg8-4XMses-aMiNpZ-o6Neth-8TxrKL-ddPsMJ-uxPQK-rnFaFc-htcz8p-htcy5x-9pDSh4-oQaKpX-f6H65d-rTYKyP-Gte91-ekYLQJ-htbd9f-sQNh1p">Quality sign</a></p>
<p><a href="https://www.flickr.com/photos/guitavares/1703252007/in/photolist-3AvBtV-3AvCQ6-nBnM-4HfejV-9uf9NP-eX5jSM-8hrNEn-2MDS5B-55KzGa-4K3cDa-8Znir-bSd2B-8Znip-5kXysN-866V38-4ghkhC-866UZg-866UWV-4w8YGS-3epvkv-8kmZfe-cJMKPJ-7vWW3E-3MHdHM-6DWz8h-cv6EhY-khUV9-mZ5ShZ-bvDsfh-5q9bjU-4bLqJM-6iZn4u-6iVeZa-4ytuaC-63ayfX-JCqgh-57Zezw-h2xk5-bXZM4L-5NgYg1-71G2g8-cDj7c-bJT262-cDj7b-4B3Sn5-6z6KJH-6zaPAN-6z6KJc-PChLE-6huwa4">Software Bug</a></p>
<p><a href="https://www.flickr.com/photos/rainchurch/6311895493/in/photolist-aBL7Cn-8kDCwc-nV5Vqt-aebusT-7Tx3yN-atDXwQ-aBz8pq-jFxbPt-fyZ2mz-atBg4t-aCntuf-589hgC-4L8zuA-atDSxE-atB16v-atDWeS-4L4vHV-LLGG-4L8vMq-jFxbKa-dURZjE-a64gqP-5Q8NTE-a64gae-a67acE-a67ary-a67a5C-a64hVr-a679uu-a64hKB-a64hkg-a67akJ-a64hD6-a6792E-a64j2r-a678k9-a678BC-a64ijB-a64jEZ-a64heR-a679RE-a64i2B-9nsZVT-9nw4a9-ak7tSd-a6798G-a64gSV-a64gYB-a67aNA-a67ayu">Safety Net</a></p>
<p><a href="https://www.flickr.com/photos/clofresh/3384877145/in/photolist-6a7o3M-3xKDX6-3vQg9R-3vMZ7L-3vHyX2-3vMXoY-3vMYCA-8T18gC-8SX3A6-8ThRyW-8SX3pi-8SX3tv-8F2xeK-vLQKs-6TMD5S-3KJ8ju-4W4bEU-5tYhfL-9imr2h-9iimcr-8PaDRY-85NjRy-6cuh21-6cuh5b-6Vy9HP-8FBc7A-7x5yiZ-9a3zB1-9zewGu-7XHaJi-98KWFi-8FuZRs-76wP1Z-76wNXF-4HMXAY-kDuJBt-kDuJjp-kDvg3X-kDvgnz-kDuKxB-aqyWws-avnoWQ-kDuKng-pmv94-8H1SkE-kDvhmi-kDvhca-kDuKGe-kDwXku-kDwWMq">Hudson Icon</a></p>
<p><a href="https://www.flickr.com/photos/adulau/9464930917/in/photolist-fqofd6-yGJ6F-oCSHz-AyGb7-oCT7x-7j6xqw-5aiWWj-5Vew2s-ziEWJ-oCT9J-8zDSxG-nhXe1-oCT6C-8QYrBh-3Putvo-aVrTfT-3RULze-7VzXR-bbdPdX-4WoxZV-4vSnu-yGFrF-4iR4MH-4rYGhX-oCT3F-4NKHig-myqTsK-oCKxT-4s3Mmd-oCT59-dpYE6B-79p1wW-9pNy3j-3R8Fyq-9GhW94-oCT4U-7oATva-8uwo-9pNw7Y-4eErsa-6bo6Kf-7RUwtv-7D7PPY-7S335j-afajTC-oCT9q-9pNuQy-6ZY5Yw-8iDkVc-Cswv3">Hacking Tents</a></p>
</section>
